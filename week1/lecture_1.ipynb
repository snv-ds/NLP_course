{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lecture_1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMwELB6DV3AUT4OxosO1dhY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"mAsyJjyNt5Iv"},"source":["import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MpF4iUEstAko"},"source":["tokenization"]},{"cell_type":"code","metadata":{"id":"Q9jA2bEmupnP"},"source":["text = 'Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGrOtDxZs9Wx"},"source":["from nltk.tokenize import WordPunctTokenizer\n","\n","WordPunctTokenizer().tokenize(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1cA45dpud-H"},"source":["from nltk.tokenize import RegexpTokenizer\n","\n","tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n","tokenizer.tokenize(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lAP7z8elud7B"},"source":["from nltk.tokenize import TweetTokenizer\n","\n","text_tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n","\n","tokenizer = TweetTokenizer()\n","tokenizer.tokenize(text_tweet)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tl66suXtwGUP"},"source":["Perform charcter tokenizer using basic Python features"]},{"cell_type":"code","metadata":{"id":"3PYinU0avkl1"},"source":["# YOUR CODE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3fyWbPkwNkq"},"source":["class CustomCharTokenizer:\n","  def __init___(self):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5Ie8fOVwvnX"},"source":["assert len(CustomCharTokenizer()(text)) == len(text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rPwoPG2ktKRR"},"source":["one hot encoding"]},{"cell_type":"code","metadata":{"id":"l-BidndetLvu"},"source":["text = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n"," Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n"," Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. \n"," Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hObc_yMkyaXZ"},"source":["tokens = set(text.split())\n","list(tokens)[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vS2VgTJIyi5I"},"source":["num2token = dict(enumerate(tokens))\n","token2num = {value:key for key, value in num2token.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PD72H5LAyi1K"},"source":["def create_ohe(word, token2num, vocab_size):\n","  vector = np.zeros((vocab_size, 1))\n","  vector[token2num[word], 0] = 1\n","  return vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"usGRqXwcyixU"},"source":["create_ohe('veniam,', token2num, len(tokens))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GP1Sac_021k"},"source":["Try to delete punctuation and other tricks to trim vocab"]},{"cell_type":"code","metadata":{"id":"TYrkD40Q12vD"},"source":["# YOUR CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AdIoQIH7tMSs"},"source":["tf-idf usage"]},{"cell_type":"code","metadata":{"id":"AIEoQkek3hdD"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgIC_AQW3mWF"},"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n","newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mTX-LLB4MFf"},"source":["vectorizer = TfidfVectorizer()\n","vectors = vectorizer.fit_transform(newsgroups_train.data)\n","vectors.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gN6Zz0dE3-Xn"},"source":["Initialize and train tf-idf vectors with 1,2 n-grams, and have maximum 30 000 features"]},{"cell_type":"code","metadata":{"id":"Hhosy62-45MV"},"source":["#YOUR CODE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEerqGX-3yFh"},"source":["assert vectors.shape == (2034, 30000)\n","assert vectors[0].todense().sum() > 9.6"],"execution_count":null,"outputs":[]}]}